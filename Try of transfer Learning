#迁移学习：在ImageNet上得到一个预训练好的ConvNet网络，删除网络顶部的全连接层，
#然后将ConvNet网络的剩余部分作为新数据集的特征提取层。这也就是说，
#我们使用了ImageNet提取到的图像特征，为新数据集训练分类器。
import numpy as np
from keras.models import model_from_json
import h5py
import os
import sys
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.utils import np_utils
import json
import argparse
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
#from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD

root_dir = "C:/Users/ksuser_2/Desktop/Type3"
categories = ["試験片なし","小划痕","擦伤与漆面不良"]
# 指定要使用的模型
model = Sequential()
in_shape=(64,64,3)
nb_classes=3
print(in_shape)
# 畳み込み層の作成
# 1層目の追加　　１０２４個の層を最初に作り、フィルター3*3のフィルターを１６個作成
model.add(Convolution2D(16, 3, 3, border_mode="same", input_shape=in_shape))
model.add(Activation("relu"))

# ２層目の畳み込み層
model.add(Convolution2D(16, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# プーリング層
model.add(MaxPooling2D())

# Dropoutとは過学習を防ぐためのもの　0.5は次のニューロンへのパスをランダムに半分にするという意味
model.add(Dropout(0.5))

# ３層目の作成
model.add(Convolution2D(32, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# ４層目の作成
model.add(Convolution2D(32, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# プーリング層
model.add(MaxPooling2D())
model.add(Dropout(0.5))

# ５層目
model.add(Convolution2D(64, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# 6層目
model.add(Convolution2D(64, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# プーリング層
model.add(MaxPooling2D())

# Dropout
model.add(Dropout(0.5))

# 7層目
model.add(Convolution2D(128, 3, 3, border_mode="same"))
model.add(Activation("relu"))

# Dropout
model.add(Dropout(0.5))

# 平坦化
model.add(Flatten())
keras_param = "C:/Users/ksuser_2/Desktop/PredataROF/64idol-model.hdf5"
model.load_weights(keras_param,by_name=True)

# 8層目　全結合層　FC
model.add(Dense(1000, name='dense1'))
model.add(Activation('relu'))
# Dropout
model.add(Dropout(0.5))

# 8層目　引数nub_classesとは分類の数を定義する。
model.add(Dense(3))
model.add(Activation('softmax'))

# ここまででモデルの層完成
# モデルのサマリーの表示
# model.summary()
# plt.plot(model, show_shapes=True, show_layer_names=True, to_file='model.png')

# lossは損失関数を定義するところ
model.compile(loss="categorical_crossentropy",
              metrics=["accuracy"],
              optimizer="adam"
              )
model.summary()
X_train, X_test, y_train, y_test = np.load("C:/Users/ksuser_2/Desktop/Type3/Type3idol.npy")
X_train = X_train.astype("float") / 256
X_test  = X_test.astype("float")  / 256
y_train = np_utils.to_categorical(y_train, nb_classes)
y_test  = np_utils.to_categorical(y_test, nb_classes)
hist=model.fit(X_train, y_train, epochs=30, batch_size=50,validation_split=0.2)

#fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，
# 如果有验证集的话，也包含了验证集的这些指标变化情况
print(hist.history)
import matplotlib.pyplot as plt
# list all data in history
print(hist.history.keys())
# summarize history for accuracy
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
#keras_model = "C:/Users/Borui/Desktop/dataset/idol.json"
#keras_param = "C:/Users/Borui/Desktop/dataset/idol-model.hdf5"
#modelVGG = model_from_json(open(keras_model).read())

#modelVGG.summary()
#modelVGG.pop()
#modelVGG.pop()
#modelVGG.pop()
##modelVGG.pop()
#modelVGG.pop()
#modelVGG.load_weights(keras_param,by_name=True)
#modelVGG.add(Dense(1000))
#modelVGG.add(Activation("relu"))

# Dropout
#modelVGG.add(Dropout(0.5))

# 8層目　引数nub_classesとは分類の数を定義する。第八层参数nub_classes定义了分类的数量。
#modelVGG.add(Dense(2))
#modelVGG.add(Activation('softmax'))

# ここまででモデルの層完成   到目前为止完成了模型层

# lossは損失関数を定義するところ 损失定义了损失函数
#modelVGG.compile(loss="categorical_crossentropy",
              #metrics=["accuracy"],
              #optimizer="adam"
                # )
#modelVGG.pop()
#for layer in modelVGG.layers[21:]:
    #layer.trainable = False
#base_model = modelVGG(inputs=modelVGG.input,include_top=False) #include_top=False excludes final FC layer
#modelVGG.summary()
